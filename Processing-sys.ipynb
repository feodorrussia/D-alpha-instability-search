{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36684487-e4c3-4787-8a13-095ae2faf6bc",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing processing system\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eb1f526-f7c8-46ec-88e9-24b7088ec652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import shtReader_py.shtRipper as shtRipper\n",
    "from source.Files_operating import read_dataFile, read_sht_data\n",
    "from source.NN_environment import process_fragments, get_borders, normalise_series, down_to_zero\n",
    "# from source.NN_environment import get_prediction_multi_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99bdd0fe-dd35-4876-a15d-73e230216014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from source.NN_environment import unet_multi_model, dice_bce_loss\n",
    "\n",
    "# TO DO: predict all fragment in one call\n",
    "def get_prediction_multi_unet(data: np.array, POINTS_DIM=1024, NUM_CLASSES=2, ckpt_v=0) -> np.array:\n",
    "    checkpoint_filepath = f'models/ckpt/checkpoint_{ckpt_v}_multi.weights.h5'\n",
    "\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    \n",
    "    model = unet_multi_model(POINTS_DIM, NUM_CLASSES)\n",
    "    model.compile(optimizer='adam', loss=dice_bce_loss,\n",
    "                  metrics=['acc', precision, recall])\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    l_edge = 0\n",
    "    step = 256\n",
    "    \n",
    "    prediction_result = np.zeros((NUM_CLASSES, data.shape[0]))\n",
    "    # print(prediction_result.shape, prediction_result)\n",
    "    \n",
    "    while l_edge + POINTS_DIM < data.shape[0]:\n",
    "        predictions = model.predict(np.array([normalise_series(data[l_edge:l_edge + POINTS_DIM])]), verbose=0)\n",
    "        # print(predictions.shape)\n",
    "        for i in range(0, POINTS_DIM):\n",
    "            prediction_result[0, l_edge + i] = predictions[0, i, 0]\n",
    "            prediction_result[1, l_edge + i] = predictions[0, i, 1]\n",
    "        l_edge += step\n",
    "    \n",
    "    if l_edge + POINTS_DIM - step != data.shape[0] - 1:\n",
    "        predictions = model.predict(np.array([normalise_series(data[data.shape[0] - POINTS_DIM:])]), verbose=0)\n",
    "        for i in range(0, POINTS_DIM):\n",
    "            prediction_result[0, data.shape[0] - POINTS_DIM + i] = predictions[0, i, 0]\n",
    "            prediction_result[1, data.shape[0] - POINTS_DIM + i] = predictions[0, i, 1]\n",
    "\n",
    "    return prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "079adf29-e6f1-4749-9c1b-6f7f01cd3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sht44350\"\n",
    "filepath = \"C:/Users/f.belous/Work/Projects/D-alpha-instability-search/data/sht/\"\n",
    "ckpt_v = 1\n",
    "\n",
    "F_ID = filename[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73ea5fa1-09cc-40bb-b827-2c2d7bd1b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sht_data(filename, filepath, data_name=\"D-alfa  хорда R=50 cm\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dd8c676-beba-4188-8e04-d3b0ce9218ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f.belous\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 52 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "predictions = get_prediction_multi_unet(df.ch1.to_numpy(), ckpt_v=ckpt_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "888837d5-e9c7-4b17-8da0-7326ea9dc190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved successfully to C:/Users/f.belous/Work/Projects/D-alpha-instability-search/data/sht/marked/44350_data.SHT\n",
      "Took - 106.0 s\n"
     ]
    }
   ],
   "source": [
    "df[\"unsync_ai_marked\"] = predictions[0, :]\n",
    "df[\"sync_ai_marked\"] = predictions[1, :]\n",
    "\n",
    "df[\"unsync_marked\"] = down_to_zero(np.array(df[\"unsync_ai_marked\"]), edge=0.5)\n",
    "df[\"unsync_marked\"] = process_fragments(np.array(df[\"ch1\"]), np.array(df[\"unsync_marked\"]), length_edge=30, scale=1.5)  # old version: length_edge=20, , scale=0\n",
    "\n",
    "df[\"sync_marked\"] = down_to_zero(np.array(df[\"sync_ai_marked\"]), edge=0.5)\n",
    "df[\"sync_marked\"] = process_fragments(np.array(df[\"ch1\"]), np.array(df[\"sync_marked\"]), length_edge=30, scale=0)  # old version: length_edge=30, , scale=1.5\n",
    "\n",
    "sxr_df = read_sht_data(filename, filepath, data_name=\"SXR 50 mkm\")\n",
    "\n",
    "comment = {\"ai_marking\": f'Processed NN prediction of ELMs (v{ckpt_v} multiclass model; trn-on: #44[184|194] )',\n",
    "           \"sync_proc_marks\": f'Sync ELMs marks (by proc-sys v2.1-0scl; {datetime.now().strftime(\"%d.%m.%Y\")})',\n",
    "           \"unsync_proc_marks\": f'Unsync ELMs marks (by proc-sys v2.2-1.5scl; {datetime.now().strftime(\"%d.%m.%Y\")})'}\n",
    "\n",
    "to_pack = {\n",
    "    \"D-alpha, chord=50 cm\": {\n",
    "        'comment': f'SHOT: #{F_ID}',\n",
    "        'unit': 'U(V)',\n",
    "        # 'x': df.t,\n",
    "        'tMin': df.t.min(),  # minimum time\n",
    "        'tMax': df.t.max(),  # maximum time\n",
    "        'offset': 0.0,  # ADC zero level offset\n",
    "        'yRes': 0.001,  # ADC resolution: 0.0001 Volt per adc bit\n",
    "        'y': df.ch1.to_list()\n",
    "    },\n",
    "    \"SXR 50 mkm\": {\n",
    "        'comment': f'ADC #4, CH #6, SHOT: #{F_ID}',\n",
    "        'unit': 'U(V)',\n",
    "        # 'x': df.t,\n",
    "        'tMin': df.t.min(),  # minimum time\n",
    "        'tMax': df.t.max(),  # maximum time\n",
    "        'offset': 0.0,  # ADC zero level offset\n",
    "        'yRes': 0.001,  # ADC resolution: 0.0001 Volt per adc bit\n",
    "        'y': sxr_df.ch1.to_list()\n",
    "    },\n",
    "    \"Unsync ELM AI prediction\": {\n",
    "        'comment': comment[\"ai_marking\"],\n",
    "        'unit': 'U(V)',\n",
    "        # 'x': df.t,\n",
    "        'tMin': df.t.min(),  # minimum time\n",
    "        'tMax': df.t.max(),  # maximum time\n",
    "        'offset': 0.0,  # ADC zero level offset\n",
    "        'yRes': 0.001,  # ADC resolution: 0.0001 Volt per adc bit\n",
    "        'y': df.unsync_ai_marked.to_list()\n",
    "    },\n",
    "    \"Unsync ELM mark\": {\n",
    "        'comment': comment[\"unsync_proc_marks\"],\n",
    "        'unit': 'U(V)',\n",
    "        # 'x': df.t,\n",
    "        'tMin': df.t.min(),  # minimum time\n",
    "        'tMax': df.t.max(),  # maximum time\n",
    "        'offset': 0.0,  # ADC zero level offset\n",
    "        'yRes': 0.001,  # ADC resolution: 0.0001 Volt per adc bit\n",
    "        'y': df.unsync_marked.to_list()\n",
    "    },\n",
    "    \"Sync ELM AI prediction\": {\n",
    "        'comment': comment[\"ai_marking\"],\n",
    "        'unit': 'U(V)',\n",
    "        # 'x': df.t,\n",
    "        'tMin': df.t.min(),  # minimum time\n",
    "        'tMax': df.t.max(),  # maximum time\n",
    "        'offset': 0.0,  # ADC zero level offset\n",
    "        'yRes': 0.001,  # ADC resolution: 0.0001 Volt per adc bit\n",
    "        'y': df.sync_ai_marked.to_list()\n",
    "    },\n",
    "    \"Sync ELM mark\": {\n",
    "        'comment': comment[\"sync_proc_marks\"],\n",
    "        'unit': 'U(V)',\n",
    "        # 'x': df.t,\n",
    "        'tMin': df.t.min(),  # minimum time\n",
    "        'tMax': df.t.max(),  # maximum time\n",
    "        'offset': 0.0,  # ADC zero level offset\n",
    "        'yRes': 0.001,  # ADC resolution: 0.0001 Volt per adc bit\n",
    "        'y': df.sync_marked.to_list()\n",
    "    }\n",
    "}\n",
    "\n",
    "packed = shtRipper.ripper.write(path=filepath + \"marked/\", filename=f'{F_ID}_data.SHT', data=to_pack)\n",
    "\n",
    "print(f\"Result saved successfully to {filepath}marked/{F_ID}_data.SHT\")\n",
    "print(f\"Took - {round(time.time() - start_time, 2)} s\")\n",
    "\n",
    "df.to_csv(filepath + f\"marked/df/{F_ID}_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9cb4f-aa8a-4e40-9ada-f7f69a1505b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
